{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning (DQN) - Cartpole Task\n",
    "\n",
    "* The goal is for agent to learn keep the pole attached to a cart upright as long as possible. The agent does so by choosing between two actions: 'move the cart left or right'.\n",
    "\n",
    "* In reinforcement learning the agent learns how to act or take action by trail and error. In other words, by trying out and action and receving a feedback/reward. It uses the feedback/reward to learn if taking an action in a particular senario (state) was good or bad. The model aim to maximise the total accumulated reward. \n",
    "\n",
    "* Adaptation of https://github.com/keon/deep-q-learning/blob/master/ddqn.py\n",
    "\n",
    "* Current version just uses CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from time import time\n",
    "from keras.callbacks import TensorBoard\n",
    "from collections import deque, namedtuple\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Multiply\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 0.99\n",
    "TARGET_UPDATE = 100\n",
    "LEARNING_RATE = 0.001\n",
    "EPISODES = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP transition represented as a named tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay memory (a database to store input data and to sample from to create a training batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.memory = deque(maxlen = capacity)\n",
    "        \n",
    "    def add(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "    \n",
    "    def sample(self, batch_size=10):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def get_memory(self):\n",
    "        return self.memory\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Huber Loss Function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _huber_loss(y_true, y_pred, clip_delta=1.0):\n",
    "    error = y_true - y_pred\n",
    "    cond  = K.abs(error) <= clip_delta\n",
    "    squared_loss = 0.5 * K.square(error)\n",
    "    quadratic_loss = 0.5 * K.square(clip_delta) + clip_delta * (K.abs(error) - clip_delta)\n",
    "    return K.mean(tf.where(cond, squared_loss, quadratic_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "states (InputLayer)             (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 24)           120         states[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 24)           600         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            50          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mask (InputLayer)               (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 2)            0           dense_3[0][0]                    \n",
      "                                                                 mask[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, state_size, action_size):\n",
    "\n",
    "        # create the neural network.\n",
    "        states_input = Input((state_size,), name='states')\n",
    "        actions_input = Input((action_size,), name='mask')\n",
    "\n",
    "        fc1 = Dense(24, activation='tanh')(states_input)\n",
    "        fc2 = Dense(24, activation='tanh')(fc1)\n",
    "        out = Dense(action_size, activation='linear')(fc2)\n",
    "        filtered_output = Multiply()([out, actions_input])\n",
    "        self.model = Model(inputs=[states_input, actions_input], outputs=filtered_output)\n",
    "        self.model.compile(loss=_huber_loss,optimizer=Adam(lr=LEARNING_RATE, clipnorm=1.))\n",
    "    \n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "        \n",
    "\n",
    "\n",
    "net = Network(4, 2)\n",
    "model = net.get_model()\n",
    "print(model.summary())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[99m Episode 0 finished after 16 steps  and with total reward 14.0\n",
      "\u001b[99m Episode 1 finished after 20 steps  and with total reward 18.0\n",
      "\u001b[99m Episode 2 finished after 21 steps  and with total reward 19.0\n",
      "\u001b[99m Episode 3 finished after 15 steps  and with total reward 13.0\n",
      "\u001b[99m Episode 4 finished after 17 steps  and with total reward 15.0\n",
      "\u001b[99m Episode 5 finished after 38 steps  and with total reward 36.0\n",
      "\u001b[99m Episode 6 finished after 63 steps  and with total reward 61.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 1ms/step - loss: 0.2598\n",
      "\u001b[99m Episode 7 finished after 30 steps  and with total reward 28.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 23us/step - loss: 0.2501\n",
      "\u001b[99m Episode 8 finished after 34 steps  and with total reward 32.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 16us/step - loss: 0.2614\n",
      "\u001b[99m Episode 9 finished after 78 steps  and with total reward 76.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 8us/step - loss: 0.2259\n",
      "\u001b[99m Episode 10 finished after 50 steps  and with total reward 48.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 16us/step - loss: 0.2161\n",
      "\u001b[99m Episode 11 finished after 27 steps  and with total reward 25.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 8us/step - loss: 0.2076\n",
      "\u001b[99m Episode 12 finished after 28 steps  and with total reward 26.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 15us/step - loss: 0.2098\n",
      "\u001b[99m Episode 13 finished after 29 steps  and with total reward 27.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 23us/step - loss: 0.1950\n",
      "\u001b[99m Episode 14 finished after 25 steps  and with total reward 23.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 16us/step - loss: 0.1866\n",
      "\u001b[99m Episode 15 finished after 28 steps  and with total reward 26.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 16us/step - loss: 0.2047\n",
      "\u001b[99m Episode 16 finished after 29 steps  and with total reward 27.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 16us/step - loss: 0.1908\n",
      "\u001b[99m Episode 17 finished after 34 steps  and with total reward 32.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 16us/step - loss: 0.1913\n",
      "\u001b[99m Episode 18 finished after 33 steps  and with total reward 31.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 8us/step - loss: 0.1797\n",
      "\u001b[99m Episode 19 finished after 25 steps  and with total reward 23.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 16us/step - loss: 0.1832\n",
      "\u001b[99m Episode 20 finished after 67 steps  and with total reward 65.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 16us/step - loss: 0.1885\n",
      "\u001b[99m Episode 21 finished after 46 steps  and with total reward 44.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 23us/step - loss: 0.1641\n",
      "\u001b[99m Episode 22 finished after 31 steps  and with total reward 29.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 23us/step - loss: 0.1781\n",
      "\u001b[99m Episode 23 finished after 35 steps  and with total reward 33.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 16us/step - loss: 0.1831\n",
      "\u001b[99m Episode 24 finished after 49 steps  and with total reward 47.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 23us/step - loss: 0.1792\n",
      "\u001b[99m Episode 25 finished after 22 steps  and with total reward 20.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 16us/step - loss: 0.1581\n",
      "\u001b[99m Episode 26 finished after 24 steps  and with total reward 22.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 16us/step - loss: 0.1776\n",
      "\u001b[99m Episode 27 finished after 51 steps  and with total reward 49.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 16us/step - loss: 0.1620\n",
      "\u001b[99m Episode 28 finished after 24 steps  and with total reward 22.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 16us/step - loss: 0.1701\n",
      "\u001b[99m Episode 29 finished after 36 steps  and with total reward 34.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 23us/step - loss: 0.1489\n",
      "\u001b[99m Episode 30 finished after 35 steps  and with total reward 33.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 16us/step - loss: 0.1575\n",
      "\u001b[99m Episode 31 finished after 27 steps  and with total reward 25.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 16us/step - loss: 0.1374\n",
      "\u001b[99m Episode 32 finished after 23 steps  and with total reward 21.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 16us/step - loss: 0.1443\n",
      "\u001b[99m Episode 33 finished after 45 steps  and with total reward 43.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 8us/step - loss: 0.1455\n",
      "\u001b[99m Episode 34 finished after 29 steps  and with total reward 27.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 23us/step - loss: 0.1342\n",
      "\u001b[99m Episode 35 finished after 31 steps  and with total reward 29.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 8us/step - loss: 0.1416\n",
      "\u001b[99m Episode 36 finished after 26 steps  and with total reward 24.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 8us/step - loss: 0.1283\n",
      "\u001b[99m Episode 37 finished after 28 steps  and with total reward 26.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 16us/step - loss: 0.1220\n",
      "\u001b[99m Episode 38 finished after 26 steps  and with total reward 24.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 15us/step - loss: 0.1293\n",
      "\u001b[99m Episode 39 finished after 34 steps  and with total reward 32.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 8us/step - loss: 0.1160\n",
      "\u001b[99m Episode 40 finished after 27 steps  and with total reward 25.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 16us/step - loss: 0.1196\n",
      "\u001b[99m Episode 41 finished after 36 steps  and with total reward 34.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 16us/step - loss: 0.1347\n",
      "\u001b[99m Episode 42 finished after 26 steps  and with total reward 24.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 15us/step - loss: 0.0926\n",
      "\u001b[99m Episode 43 finished after 44 steps  and with total reward 42.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 16us/step - loss: 0.1109\n",
      "\u001b[99m Episode 44 finished after 24 steps  and with total reward 22.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 16us/step - loss: 0.1227\n",
      "\u001b[99m Episode 45 finished after 41 steps  and with total reward 39.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 16us/step - loss: 0.1136\n",
      "\u001b[99m Episode 46 finished after 50 steps  and with total reward 48.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 16us/step - loss: 0.0909\n",
      "\u001b[99m Episode 47 finished after 27 steps  and with total reward 25.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 8us/step - loss: 0.1002\n",
      "\u001b[99m Episode 48 finished after 32 steps  and with total reward 30.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 16us/step - loss: 0.0962\n",
      "\u001b[99m Episode 49 finished after 38 steps  and with total reward 36.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 8us/step - loss: 0.0888\n",
      "\u001b[99m Episode 50 finished after 29 steps  and with total reward 27.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 8us/step - loss: 0.0817\n",
      "\u001b[99m Episode 51 finished after 50 steps  and with total reward 48.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 15us/step - loss: 0.0902\n",
      "\u001b[99m Episode 52 finished after 35 steps  and with total reward 33.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 16us/step - loss: 0.0743\n",
      "\u001b[99m Episode 53 finished after 34 steps  and with total reward 32.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 8us/step - loss: 0.0820\n",
      "\u001b[99m Episode 54 finished after 31 steps  and with total reward 29.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 8us/step - loss: 0.1025\n",
      "\u001b[99m Episode 55 finished after 25 steps  and with total reward 23.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 16us/step - loss: 0.0552\n",
      "\u001b[99m Episode 56 finished after 35 steps  and with total reward 33.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 8us/step - loss: 0.0827\n",
      "\u001b[99m Episode 57 finished after 24 steps  and with total reward 22.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 8us/step - loss: 0.0531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[99m Episode 58 finished after 58 steps  and with total reward 56.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 8us/step - loss: 0.0594\n",
      "\u001b[99m Episode 59 finished after 39 steps  and with total reward 37.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 16us/step - loss: 0.0786\n",
      "\u001b[99m Episode 60 finished after 31 steps  and with total reward 29.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 16us/step - loss: 0.0504\n",
      "\u001b[99m Episode 61 finished after 40 steps  and with total reward 38.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 16us/step - loss: 0.0637\n",
      "\u001b[99m Episode 62 finished after 44 steps  and with total reward 42.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 16us/step - loss: 0.0710\n",
      "\u001b[99m Episode 63 finished after 49 steps  and with total reward 47.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 16us/step - loss: 0.0576\n",
      "\u001b[99m Episode 64 finished after 32 steps  and with total reward 30.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 16us/step - loss: 0.0386\n",
      "\u001b[99m Episode 65 finished after 43 steps  and with total reward 41.0\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 8us/step - loss: 0.0505\n"
     ]
    }
   ],
   "source": [
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        #self.tensorboard = TensorBoard(log_dir='logs/{}'.format(time()))\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.network = Network(self.state_size, self.action_size)\n",
    "        self.target_net = Network(self.state_size, self.action_size)\n",
    "        self.memory = ReplayMemory()\n",
    "        self.steps = 0\n",
    "        self.epsilon = EPS_START\n",
    "        self.duration = [0] * 100\n",
    "\n",
    "    def update_target_model(self):\n",
    "        # copy weights from model to target_model\n",
    "        self.target_net.get_model().set_weights(self.network.get_model().get_weights())\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        if self.epsilon > EPS_END:\n",
    "            self.epsilon *= EPS_DECAY        \n",
    "        self.steps += 1\n",
    "        #Epsilon greedy exploration/exploitation.\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.network.get_model().predict([state, np.ones((1,2))])\n",
    "        return np.argmax(act_values[0])\n",
    "    \n",
    "    def exploit_action(self, state):\n",
    "        act_values = self.network.get_model().predict([state, np.ones((1,2))])\n",
    "        return np.argmax(act_values[0])\n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        #Check if we have generated enough data to train.\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        \n",
    "        #sample the minibatch to train network on.\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        state_batch = np.asarray(batch.state).reshape(BATCH_SIZE,self.state_size)\n",
    "        action_batch = np.asarray(batch.action).reshape(BATCH_SIZE,-1)\n",
    "        reward_batch = np.asarray(batch.reward).reshape(BATCH_SIZE,-1)\n",
    "        next_state_batch = np.asarray(batch.next_state).reshape(BATCH_SIZE,self.state_size)\n",
    "        done_batch = np.asarray(batch.done).reshape(BATCH_SIZE,-1)\n",
    "        \n",
    "        #one hot encoding of action space.\n",
    "        one_hot_targets = (np.eye(self.action_size)[action_batch]).reshape(BATCH_SIZE,-1)\n",
    "        \n",
    "        # Compute max V(s_{t+1}) for all next states.\n",
    "        next_state_values = self.target_net.get_model().predict([next_state_batch, np.ones(one_hot_targets.shape)])\n",
    "        \n",
    "        # Compute the expected Q values\n",
    "        end_multiplier = -(done_batch - 1) \n",
    "        expected_state_action_values = ((np.max(next_state_values, axis=1, keepdims=True) * GAMMA)*end_multiplier) + reward_batch\n",
    "        # Fit the keras model.\n",
    "        model.fit([state_batch, one_hot_targets], one_hot_targets * expected_state_action_values,\n",
    "                   epochs=1, batch_size=BATCH_SIZE, verbose=1)\n",
    "        #callbacks=[self.tensorboard]  \n",
    "        \n",
    "    def run(self, episode_no):\n",
    "        #reset the environment to start a fresh trial.\n",
    "        state = self.env.reset()\n",
    "        state = np.reshape(state, [1, self.state_size])\n",
    "        self.steps = 0\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            action = self.select_action(state)\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, self.state_size])\n",
    "            if done and self.steps < 195:\n",
    "                reward = -1\n",
    "            total_reward += reward\n",
    "            #add transition to replay memory.\n",
    "            self.memory.add(state, action, next_state, reward, done)\n",
    "            \n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                print(\"{2} Episode {0} finished after {1} steps  and with total reward {3}\"\n",
    "                      .format(episode_no, self.steps, '\\033[92m' if self.steps >= 195 else '\\033[99m',\n",
    "                              total_reward))\n",
    "                break\n",
    "        self.duration[(episode_no%100)] = self.steps\n",
    "        #train for mini batch.\n",
    "        self.train()\n",
    "\n",
    "        if episode_no % TARGET_UPDATE == 0:\n",
    "            self.update_target_model()\n",
    "\n",
    "    def test(self, number_of_episodes):\n",
    "        for e in range(number_of_episodes):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            t = 0\n",
    "            total_reward = 0\n",
    "            while True:\n",
    "                t += 1\n",
    "                self.env.render()\n",
    "                action = self.exploit_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                state = np.reshape(next_state, [1, self.state_size])\n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            print(\"number of steps : \" + str(t))\n",
    "            print(\"total_reward : \" + str(total_reward))\n",
    "            \n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "agent = DQN()\n",
    "\n",
    "for i in range(500):\n",
    "    agent.run(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.test(10)\n",
    "print('Complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.network.get_model().save_weights(\"cart.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
